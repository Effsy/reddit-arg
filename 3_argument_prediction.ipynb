{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Prediction\n",
    "\n",
    "In this notebook, the argument prediction model is trained.\n",
    "\n",
    "This includes the baseline and the ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "\n",
    "import json\n",
    "\n",
    "dataset = []\n",
    "\n",
    "with open('./labelled_data/1000_labelled_argument_sentences_3.json') as f:\n",
    "    for line in f:\n",
    "        json_line = json.loads(line)\n",
    "        arg = {\"text\": json_line[\"content\"], \"label\": json_line[\"annotation\"][\"labels\"][0]}\n",
    "\n",
    "        dataset.append(arg)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "\n",
    "import string\n",
    "\n",
    "print(string.punctuation)\n",
    "\n",
    "dataset = [{\"text\": sample[\"text\"].translate(str.maketrans('', '', string.punctuation)), \"label\":sample[\"label\"]} for sample in dataset]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Original dataset\n",
    "train, test = train_test_split(dataset, test_size=0.1)\n",
    "\n",
    "train_x = [sample[\"text\"] for sample in train]\n",
    "train_y = [sample[\"label\"] for sample in train]\n",
    "\n",
    "test_x = [sample[\"text\"] for sample in test]\n",
    "test_y = [sample[\"label\"] for sample in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Hyperparameters\n",
    "\n",
    "Find the best parameters for each classifier using an exhaustive search. Randomised search was first used to narrow the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to get undefined f1-score as in an exhaustive search (grid search), \n",
    "# some labels may never be predicted. This can lead to 0 precision or recall.\n",
    "# Ignore these warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Lemmatize all words in a sentence. Uses PoS to identify lemma\n",
    "# Function taken from: https://stackoverflow.com/a/39498745\n",
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        else:\n",
    "            yield word\n",
    "\n",
    "#Custom Transformer that lemmatizes samples in the dataset\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class LemmaTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, use_lemma=True):\n",
    "        self.use_lemma = use_lemma\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.use_lemma:\n",
    "            return [' '.join(lemmatize_all(sample)) for sample in X]\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "best_parameters_svm = {'clf__C': [10],  \n",
    "              'clf__gamma': [0.15], \n",
    "              'clf__kernel': ['rbf'],\n",
    "                'vect__stop_words': [None],\n",
    "                 'vect__ngram_range': [(1, 1)],\n",
    "                 'vect__norm': [None],\n",
    "                 'vect__use_idf': [False],\n",
    "                 'lemma__use_lemma': [True]}\n",
    "\n",
    "parameters_svm = {'clf__C': [0.1, 1, 10, 100, 10000],  \n",
    "              'clf__gamma': [0.1, 0.15, 0.3], \n",
    "              'clf__kernel': ['rbf', 'linear', 'sigmoid'],\n",
    "                'vect__stop_words': ['english', None],\n",
    "                 'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'vect__norm': ['l2', None],\n",
    "                 'vect__use_idf': [True, False],\n",
    "                 'lemma__use_lemma': [True, False]}\n",
    "\n",
    "pipeline_svm = Pipeline([('lemma', LemmaTransformer()), ('vect', TfidfVectorizer()), ('clf', SVC())])\n",
    "\n",
    "clf_svm = GridSearchCV(pipeline_svm, parameters_svm, cv=StratifiedKFold(n_splits=3, random_state=999), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "clf_svm.fit(train_x, train_y)\n",
    "\n",
    "clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "best_parameters_nb = {'clf__alpha': [0.1],\n",
    "                'vect__stop_words': [None],\n",
    "                 'vect__ngram_range': [(1, 1)],\n",
    "                 'vect__norm': ['l2'],\n",
    "                 'vect__use_idf': [False],\n",
    "                 'lemma__use_lemma': [False]}\n",
    "\n",
    "parameters_nb = {'clf__alpha': [0.5, 0.75, 0.1, 0.075, 0.05],\n",
    "                'vect__stop_words': ['english', None],\n",
    "                 'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'vect__norm': ['l2', None],\n",
    "                 'vect__use_idf': [True, False],\n",
    "                 'lemma__use_lemma': [True, False]}\n",
    "    \n",
    "pipeline_nb = Pipeline([('lemma', LemmaTransformer()), ('vect', TfidfVectorizer()), ('clf', MultinomialNB())])\n",
    "\n",
    "clf_nb = GridSearchCV(pipeline_nb, parameters_nb, cv=StratifiedKFold(n_splits=3, random_state=999), scoring='f1_macro',  n_jobs=-1)\n",
    "\n",
    "clf_nb.fit(train_x, train_y)\n",
    "\n",
    "clf_nb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "best_parameters_dec = {'clf__criterion':['gini'],\n",
    "                  'clf__max_depth': [5],\n",
    "                  'clf__max_features': [None],\n",
    "                'vect__stop_words': [None],\n",
    "                 'vect__ngram_range': [(1, 1)],\n",
    "                 'vect__norm': [None],\n",
    "                 'vect__use_idf': [True],\n",
    "                 'lemma__use_lemma': [True]}\n",
    "\n",
    "parameters_dec = {'clf__criterion':['gini','entropy'],\n",
    "                  'clf__max_depth': range(4, 10),\n",
    "                  'clf__max_features': ['sqrt', 'log2', None],\n",
    "                'vect__stop_words': ['english', None],\n",
    "                 'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'vect__norm': ['l2', None],\n",
    "                 'vect__use_idf': [True, False],\n",
    "                 'lemma__use_lemma': [True, False]}\n",
    "\n",
    "pipeline_dec = Pipeline([('lemma', LemmaTransformer()), ('vect', TfidfVectorizer()), ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "clf_dec = GridSearchCV(pipeline_dec, parameters_dec, cv=StratifiedKFold(n_splits=3, random_state=999), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "clf_dec.fit(train_x, train_y)\n",
    "\n",
    "clf_dec.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "best_parameters_log = {'clf__penalty': ['l2'],\n",
    "                  'clf__C': [10000],\n",
    "                'vect__stop_words': [None],\n",
    "                 'vect__ngram_range': [(1, 2)],\n",
    "                 'vect__norm': ['l2'],\n",
    "                 'vect__use_idf': [True],\n",
    "                 'lemma__use_lemma': [False]}\n",
    "\n",
    "parameters_log = {'clf__penalty': ['l1', 'l2'],\n",
    "                  'clf__C': [1000, 10000, 100000],\n",
    "                'vect__stop_words': ['english', None],\n",
    "                 'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'vect__norm': ['l2', None],\n",
    "                 'vect__use_idf': [True, False],\n",
    "                 'lemma__use_lemma': [True, False]}\n",
    "\n",
    "pipeline_log = Pipeline([('lemma', LemmaTransformer()), ('vect', TfidfVectorizer()), ('clf', LogisticRegression())])\n",
    "\n",
    "clf_log = GridSearchCV(pipeline_log, parameters_log, cv=StratifiedKFold(n_splits=3, random_state=999), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "clf_log.fit(train_x, train_y)\n",
    "\n",
    "clf_log.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "best_parameters_dum = {'dum__strategy': ['uniform'], \n",
    "                'vect__stop_words': [None],\n",
    "                 'vect__ngram_range': [(1, 2)],\n",
    "                 'vect__norm': [None],\n",
    "                 'vect__use_idf': [True],\n",
    "                 'lemma__use_lemma': [True]}\n",
    "\n",
    "parameters_dum = {'dum__strategy': ['stratified', 'most_frequent', 'prior', 'uniform'], \n",
    "                'vect__stop_words': ['english', None],\n",
    "                 'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'vect__norm': ['l2', None],\n",
    "                 'vect__use_idf': [True],\n",
    "                 'lemma__use_lemma': [True]}\n",
    "\n",
    "pipeline_dum = Pipeline([('lemma', LemmaTransformer()), ('vect', TfidfVectorizer()), ('dum', DummyClassifier())])\n",
    "\n",
    "clf_dum = GridSearchCV(pipeline_dum, parameters_dum, cv=StratifiedKFold(n_splits=3, random_state=999), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "clf_dum.fit(train_x, train_y)\n",
    "\n",
    "clf_dum.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"SVM\")\n",
    "print (classification_report(test_y, clf_svm.predict(test_x)))\n",
    "\n",
    "print(\"Naive Bayes\")\n",
    "print (classification_report(test_y, clf_nb.predict(test_x)))\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "print (classification_report(test_y, clf_dec.predict(test_x)))\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "print (classification_report(test_y, clf_log.predict(test_x)))\n",
    "\n",
    "print(\"Dummy Classifier\")\n",
    "print (classification_report(test_y, clf_dum.predict(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"SVM\")\n",
    "print(\"Test Score: \" + str(metrics.f1_score(test_y, clf_svm.predict(test_x), average='macro')))\n",
    "print(\"Best Score: \" + str(clf_svm.best_score_))\n",
    "\n",
    "print(\"Naive Bayes\")\n",
    "print(\"Test Score: \" + str(metrics.f1_score(test_y, clf_nb.predict(test_x), average='macro')))\n",
    "print(\"Best Score: \" + str(clf_nb.best_score_))\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "print(\"Test Score: \" + str(metrics.f1_score(test_y, clf_dec.predict(test_x), average='macro')))\n",
    "print(\"Best Score: \" + str(clf_dec.best_score_))\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Test Score: \" + str(metrics.f1_score(test_y, clf_log.predict(test_x), average='macro')))\n",
    "print(\"Best Score: \" + str(clf_log.best_score_))\n",
    "\n",
    "print(\"Dummy\")\n",
    "print(\"Test Score: \" + str(metrics.f1_score(test_y, clf_dum.predict(test_x), average='macro')))\n",
    "print(\"Best Score: \" + str(clf_dum.best_score_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Model\n",
    "\n",
    "As SVM performed the best, we will use this as a baseline. We will now explore adding different features to improve the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Build the training set. The training set uses the cross-validated predictions of the first model. This is to avoid overfitting of the second model, as it would effectively have seen the data twice.\n",
    "\n",
    "The actual classifier trained on the ngrams will include all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using the classifier trained on ngrams\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Rebuild the best classifier with the best parameters\n",
    "best_pipeline_svm = Pipeline([('lemma', LemmaTransformer(use_lemma=True)), ('vect', TfidfVectorizer(stop_words=None, ngram_range=(1, 1), norm=None, use_idf=False)), ('clf', SVC(C=10, gamma=0.15, kernel='rbf'))])\n",
    "\n",
    "# Use cross-validated predictions to avoid overfitting\n",
    "cross_val_predictions = cross_val_predict(best_pipeline_svm, train_x, train_y, cv=3)\n",
    "\n",
    "ngram_predictions = [1 if prediction == \"arg\" else 0 for prediction in cross_val_predictions]\n",
    "ngram_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Test sentiment extraction\n",
    "sentiment = [sid.polarity_scores(sample)['compound'] for sample in train_x]\n",
    "sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence length\n",
    "\n",
    "# Test sentence length extraction\n",
    "sentence_length = [len(word_tokenize(sample)) for sample in train_x]\n",
    "sentence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoS\n",
    "\n",
    "pos_counts = []\n",
    "\n",
    "for sample in train_x:\n",
    "    nn = 0\n",
    "    vb = 0\n",
    "    jj = 0\n",
    "    for word, pos in pos_tag(word_tokenize(sample)):\n",
    "        if pos.startswith('NN'):\n",
    "            nn += 1\n",
    "        elif pos.startswith('VB'):\n",
    "            vb += 1\n",
    "        elif pos.startswith('JJ'):\n",
    "            jj += 1\n",
    "            \n",
    "    pos_counts.append([nn, vb, jj])\n",
    "\n",
    "    \n",
    "pos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Precompute the features to increase grid-search efficiency\n",
    "train_x_all_features = np.column_stack([ngram_predictions, sentiment, sentence_length, pos_counts])\n",
    "train_x_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from a dataset\n",
    "def extract_features(data_x):\n",
    "    \n",
    "    # Predictions based on ngrams from classifier trained on all data\n",
    "    ngram_predictions = [1 if prediction == \"arg\" else 0 for prediction in clf_svm.predict(data_x)]\n",
    "    \n",
    "    # Sentiment of sentence\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment = [sid.polarity_scores(sample)['compound'] for sample in data_x]\n",
    "    \n",
    "    # Sentence Length\n",
    "    sentence_length = [len(word_tokenize(sample)) for sample in data_x]\n",
    "    \n",
    "    # Number of each parts of speech\n",
    "    pos_counts = []\n",
    "    for sample in data_x:\n",
    "        nn = 0\n",
    "        vb = 0\n",
    "        jj = 0\n",
    "        for word, pos in pos_tag(word_tokenize(sample)):\n",
    "            if pos.startswith('NN'):\n",
    "                nn += 1\n",
    "            elif pos.startswith('VB'):\n",
    "                vb += 1\n",
    "            elif pos.startswith('JJ'):\n",
    "                jj += 1\n",
    "        pos_counts.append([nn, vb, jj])\n",
    "    \n",
    "    # Combine features into numpy matrix\n",
    "    return np.column_stack([ngram_predictions, sentiment, sentence_length, pos_counts])\n",
    "\n",
    "test_x_all_features = extract_features(test_x)\n",
    "test_x_all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import sklearn.utils.validation\n",
    "\n",
    "parameters_stack_svm = {'clf__C': [100, 10, 1],\n",
    "                        'clf__gamma': [0.001, 0.01, 0.1],\n",
    "                        'clf__kernel': ['rbf', 'linear', 'sigmoid'],\n",
    "                        'nor': [None, Normalizer(norm='l1'), Normalizer(norm='l2'), Normalizer(norm='max')]}\n",
    "\n",
    "pipeline_stack_svm = Pipeline([('nor', Normalizer()), ('clf', SVC())])\n",
    "\n",
    "clf_stack_svm = GridSearchCV(pipeline_stack_svm, parameters_stack_svm, cv=StratifiedKFold(n_splits=3, random_state=998), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "clf_stack_svm.fit(train_x_all_features, train_y)\n",
    "\n",
    "clf_stack_svm.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "parameters_stack_nb = {'clf__var_smoothing': [0.00001, 0.000001, 0.0000001],\n",
    "                       'nor': [None, Normalizer(norm='l1'), Normalizer(norm='l2'), Normalizer(norm='max')]}\n",
    "    \n",
    "pipeline_stack_nb = Pipeline([('nor', Normalizer()), ('clf', GaussianNB())])\n",
    "\n",
    "clf_stack_nb = GridSearchCV(pipeline_stack_nb, parameters_stack_nb, cv=StratifiedKFold(n_splits=3, random_state=998), scoring='f1_macro',  n_jobs=-1)\n",
    "\n",
    "clf_stack_nb.fit(train_x_all_features, train_y)\n",
    "\n",
    "clf_stack_nb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "parameters_stack_dec = {'clf__criterion':['gini','entropy'],\n",
    "                        'clf__max_depth': range(2, 10),\n",
    "                        'clf__max_features': ['sqrt', 'log2', None],\n",
    "                        'nor': [None, Normalizer(norm='l1'), Normalizer(norm='l2'), Normalizer(norm='max')]}\n",
    "pipeline_stack_dec = Pipeline([('nor', Normalizer()), ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "clf_stack_dec = GridSearchCV(pipeline_stack_dec, parameters_stack_dec, cv=StratifiedKFold(n_splits=3, random_state=998), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "clf_stack_dec.fit(train_x_all_features, train_y)\n",
    "\n",
    "clf_stack_dec.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "parameters_stack_log = {'clf__penalty': ['l1', 'l2'],\n",
    "                        'clf__C': [0.01, 0.1, 1, 10],\n",
    "                        'nor': [None, Normalizer(norm='l1'), Normalizer(norm='l2'), Normalizer(norm='max')]}\n",
    "\n",
    "pipeline_stack_log = Pipeline([('nor', Normalizer()), ('clf', LogisticRegression())])\n",
    "\n",
    "clf_stack_log = GridSearchCV(pipeline_stack_log, parameters_stack_log, cv=StratifiedKFold(n_splits=3, random_state=998), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "clf_stack_log.fit(train_x_all_features, train_y)\n",
    "\n",
    "clf_stack_log.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"SVM\")\n",
    "print(classification_report(test_y, clf_stack_svm.predict(test_x_all_features)))\n",
    "\n",
    "print(\"Naive Bayes\")\n",
    "print(classification_report(test_y, clf_stack_nb.predict(test_x_all_features)))\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "print(classification_report(test_y, clf_stack_dec.predict(test_x_all_features)))\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "print(classification_report(test_y, clf_stack_log.predict(test_x_all_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"SVM\")\n",
    "print(\"Test Score: \" + str(metrics.f1_score(test_y, clf_stack_svm.predict(test_x_all_features), average='macro')))\n",
    "print(\"Best Score: \" + str(clf_stack_svm.best_score_))\n",
    "\n",
    "print(\"Naive Bayes\")\n",
    "print(\"Test Score: \" + str(metrics.f1_score(test_y, clf_stack_nb.predict(test_x_all_features), average='macro')))\n",
    "print(\"Best Score: \" + str(clf_stack_nb.best_score_))\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "print(\"Test Score: \" + str(metrics.f1_score(test_y, clf_stack_dec.predict(test_x_all_features), average='macro')))\n",
    "print(\"Best Score: \" + str(clf_stack_dec.best_score_))\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Test Score: \" + str(metrics.f1_score(test_y, clf_stack_log.predict(test_x_all_features), average='macro')))\n",
    "print(\"Best Score: \" + str(clf_stack_log.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild the best models using all the data\n",
    "\n",
    "In theory, this maximises the data and should improve results. Statistical claims cannot be made on the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets including all data\n",
    "\n",
    "train_x_all_data = [sample[\"text\"] for sample in dataset]\n",
    "train_y_all_data = [sample[\"label\"] for sample in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the data in a preprocessing step. joblib does not save custom classes in pipelines\n",
    "\n",
    "train_x_lemma_all_data = [' '.join(lemmatize_all(sentence)) for sentence in train_x_all_data]\n",
    "train_x_lemma_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the weak learner with the best parameters but use all the data\n",
    "\n",
    "ngram_model = Pipeline([('vect', TfidfVectorizer(stop_words=None, ngram_range=(1, 1), norm=None, use_idf=False)), ('clf', SVC(C=10, gamma=0.15, kernel='rbf'))])\n",
    "\n",
    "ngram_model.fit(train_x_lemma_all_data, train_y_all_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all the data\n",
    "\n",
    "# Use cross-validated predictions to avoid overfitting\n",
    "cross_val_predictions_all_data = cross_val_predict(ngram_model, train_x_all_data, train_y_all_data, cv=3)\n",
    "\n",
    "ngram_predictions_all_data = [1 if prediction == \"arg\" else 0 for prediction in cross_val_predictions_all_data]\n",
    "sentiment_all_data = [sid.polarity_scores(sample)['compound'] for sample in train_x_all_data]\n",
    "sentence_length_all_data = [len(word_tokenize(sample)) for sample in train_x_all_data]\n",
    "pos_counts_all_data = []\n",
    "\n",
    "for sample in train_x_all_data:\n",
    "    nn = 0\n",
    "    vb = 0\n",
    "    jj = 0\n",
    "    for word, pos in pos_tag(word_tokenize(sample)):\n",
    "        if pos.startswith('NN'):\n",
    "            nn += 1\n",
    "        elif pos.startswith('VB'):\n",
    "            vb += 1\n",
    "        elif pos.startswith('JJ'):\n",
    "            jj += 1\n",
    "            \n",
    "    pos_counts_all_data.append([nn, vb, jj])\n",
    "    \n",
    "train_x_all_features_all_data = np.column_stack([ngram_predictions_all_data, sentiment_all_data, sentence_length_all_data, pos_counts_all_data])\n",
    "train_x_all_features_all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the meta model using all the data\n",
    "meta_model = Pipeline([('nor', Normalizer(norm='max')), ('clf', DecisionTreeClassifier(criterion='gini', max_depth=3, max_features=None))])\n",
    "\n",
    "meta_model.fit(train_x_all_features_all_data, train_y_all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Save the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "import pickle\n",
    "\n",
    "joblib.dump(ngram_model, \"./models/arg_prediction_ngram_model.pkl\")\n",
    "joblib.dump(meta_model, \"./models/arg_prediction_meta_model.pkl\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
