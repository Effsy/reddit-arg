{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "In this notebook, EDA is carried out in order to understand the data. This will assist with feature extraction for the argumention prediction classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "\n",
    "import json\n",
    "\n",
    "data = []\n",
    "\n",
    "with open('./labelled_data/1000_labelled_argument_sentences.json') as f:\n",
    "    for line in f:\n",
    "        json_line = json.loads(line)\n",
    "        arg = {\"text\": json_line[\"content\"], \"label\": json_line[\"annotation\"][\"labels\"][0]}\n",
    "\n",
    "        data.append(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document term matrices (dtm) and corpus\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Corpus\n",
    "df_corpus = pd.DataFrame().from_dict(data, orient='columns')\n",
    "\n",
    "# dtm without stop words\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "df_cv = cv.fit_transform(df_corpus.text)\n",
    "df_dtm = pd.DataFrame(df_cv.toarray(), columns=cv.get_feature_names())\n",
    "df_dtm.index = df_corpus.index\n",
    "\n",
    "# dtm with stop words\n",
    "cv_all = CountVectorizer()\n",
    "df_cv_all = cv_all.fit_transform(df_corpus.text)\n",
    "df_dtm_all = pd.DataFrame(df_cv_all.toarray(), columns=cv_all.get_feature_names())\n",
    "df_dtm_all.index = df_corpus.index\n",
    "\n",
    "display(df_corpus.head())\n",
    "display(df_dtm.head())\n",
    "display(df_dtm_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of arg and not_arg\n",
    "number_of_arg = len(df_corpus[df_corpus[\"label\"] == \"arg\"])\n",
    "number_of_not_arg = len(df_corpus[df_corpus[\"label\"] == \"not_arg\"])\n",
    "\n",
    "display(number_of_arg)\n",
    "display(number_of_not_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of words in each sentence of arg and not_arg \n",
    "\n",
    "dtm_arg = df_dtm.loc[df_corpus.index[df_corpus[\"label\"] == \"arg\"].tolist()]\n",
    "dtm_notarg = df_dtm.loc[df_corpus.index[df_corpus[\"label\"] == \"not_arg\"].tolist()]\n",
    "\n",
    "print(\"Average number of words in arg sentences\")\n",
    "display(dtm_arg.sum(axis=1).mean())\n",
    "\n",
    "print(\"Average number of words in not_arg sentences\")\n",
    "display(dtm_notarg.sum(axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Words - Quantitative\n",
    "\n",
    "display(dtm_arg.sum().sort_values(ascending=False).head(20))\n",
    "display(dtm_notarg.sum().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Words - Visual\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import wordcloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_arg = df_corpus[df_corpus[\"label\"] == \"arg\"]\n",
    "df_notarg = df_corpus[df_corpus[\"label\"] == \"not_arg\"]\n",
    "concat_arg = \" \".join(arg for arg in df_arg.text)\n",
    "concat_notarg = \" \".join(arg for arg in df_notarg.text)\n",
    "\n",
    "wordcloud_arg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(concat_arg)\n",
    "wordcloud_notarg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(concat_notarg)\n",
    "\n",
    "plt.imshow(wordcloud_arg, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(\"most common words: arg\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(wordcloud_notarg, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(\"most common words: not_arg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full vocabulary of dataset and number of occurances\n",
    "print(pd.DataFrame(df_dtm.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of words in each sentence of arg and not_arg \n",
    "\n",
    "dtm_arg_all = df_dtm_all.loc[df_corpus.index[df_corpus[\"label\"] == \"arg\"].tolist()]\n",
    "dtm_notarg_all = df_dtm_all.loc[df_corpus.index[df_corpus[\"label\"] == \"not_arg\"].tolist()]\n",
    "\n",
    "print(\"Average number of words in arg sentences\")\n",
    "display(dtm_arg_all.sum(axis=1).mean())\n",
    "\n",
    "print(\"Average number of words in not_arg sentences\")\n",
    "display(dtm_notarg_all.sum(axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Words - Quantitative\n",
    "display(dtm_arg_all.sum().sort_values(ascending=False).head(20))\n",
    "display(dtm_notarg_all.sum().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Words - Visual\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import wordcloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_arg = df_corpus[df_corpus[\"label\"] == \"arg\"]\n",
    "df_notarg = df_corpus[df_corpus[\"label\"] == \"not_arg\"]\n",
    "concat_arg = \" \".join(arg for arg in df_arg.text)\n",
    "concat_notarg = \" \".join(arg for arg in df_notarg.text)\n",
    "\n",
    "wordcloud_arg = WordCloud(stopwords=None, max_font_size=50, max_words=100, background_color=\"white\").generate(concat_arg)\n",
    "wordcloud_notarg = WordCloud(stopwords=None, max_font_size=50, max_words=100, background_color=\"white\").generate(concat_notarg)\n",
    "\n",
    "plt.imshow(wordcloud_arg, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(\"most common words: arg\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(wordcloud_notarg, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(\"most common words: not_arg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full vocabulary of dataset and number of occurances\n",
    "print(pd.DataFrame(df_dtm.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraction of sentences that contain numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_numbers(text):\n",
    "    return any(char.isdigit() for char in text)\n",
    "\n",
    "print(\"Fraction of sentences that contain numbers: arg\")\n",
    "display(sum([contains_numbers(sentence) for sentence in df_corpus[df_corpus[\"label\"] == \"arg\"].text.tolist()])/number_of_arg)\n",
    "\n",
    "print(\"Fraction of sentences that contain numbers: not_arg\")\n",
    "display(sum([contains_numbers(sentence) for sentence in df_corpus[df_corpus[\"label\"] == \"not_arg\"].text.tolist()])/number_of_not_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentiment = [sid.polarity_scores(sentence)['compound'] for sentence in df_corpus[df_corpus[\"label\"] == \"arg\"].text.tolist()]\n",
    "print(\"Average sentiment in arg sentences\")\n",
    "display(sum(sentiment)/len(sentiment))\n",
    "\n",
    "sentiment = [sid.polarity_scores(sentence)['compound'] for sentence in df_corpus[df_corpus[\"label\"] == \"not_arg\"].text.tolist()]\n",
    "print(\"Average sentiment in not_arg sentences\")\n",
    "display(sum(sentiment)/len(sentiment))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "pos_counts_arg = []\n",
    "\n",
    "for sentence in df_corpus[df_corpus[\"label\"] == \"arg\"].text.tolist():\n",
    "    nn = 0\n",
    "    vb = 0\n",
    "    jj = 0\n",
    "    for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "        if pos.startswith('NN'):\n",
    "            nn += 1\n",
    "        elif pos.startswith('VB'):\n",
    "            vb += 1\n",
    "        elif pos.startswith('JJ'):\n",
    "            jj += 1\n",
    "            \n",
    "    pos_counts_arg.append([nn, vb, jj])\n",
    "\n",
    "\n",
    "pos_counts_arg = numpy.mean(pos_counts_arg, axis=0)\n",
    "\n",
    "print(\"Average number of PoS of NN (Nouns), VB (Verbs), JJ (Adjectives) in arg sentences\")\n",
    "print(pos_counts_arg)\n",
    "print(\"Normalised\")\n",
    "print(normalize([pos_counts_arg], norm='l1'))\n",
    "\n",
    "pos_counts_notarg = []\n",
    "\n",
    "for sentence in df_corpus[df_corpus[\"label\"] == \"not_arg\"].text.tolist():\n",
    "    nn = 0\n",
    "    vb = 0\n",
    "    jj = 0\n",
    "    for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "        if pos.startswith('NN'):\n",
    "            nn += 1\n",
    "        elif pos.startswith('VB'):\n",
    "            vb += 1\n",
    "        elif pos.startswith('JJ'):\n",
    "            jj += 1\n",
    "            \n",
    "    pos_counts_notarg.append([nn, vb, jj])\n",
    "\n",
    "pos_counts_notarg = numpy.mean(pos_counts_notarg, axis=0)\n",
    "\n",
    "print(\"Average number of PoS of NN (Nouns), VB (Verbs), JJ (Adjectives) in not_arg sentences\")\n",
    "print(pos_counts_notarg)\n",
    "print(\"Normalised\")\n",
    "print(normalize([pos_counts_notarg], norm='l1'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
