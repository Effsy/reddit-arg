{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data - Relations Prediction\n",
    "In this notebook, data is fetched from Reddit in order to construct an unlabelled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch & Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Replace utf-8 single quotes with ascii apostrophes\n",
    "    text = re.sub(r\"(\\u2018|\\u2019)\", \"'\", text)\n",
    "    # Replace utf-8 double quotes with ascii double quotes\n",
    "    text = re.sub(r\"(\\u201c|\\u201d)\", '\"', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_sentences(sentences):\n",
    "    # Convert common utf-8 punctuation to ascii\n",
    "    sentences = [clean_text(sentence) for sentence in sentences]\n",
    "    # Split all sentences containing \\n into separate sentences\n",
    "    sentences = [split_args for sentence in sentences for split_args in sentence.splitlines()]\n",
    "    # Remove whitespace from arguments and empty strings from thread list\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    # Remove duplicates from the list\n",
    "    sentences = list(set(sentences))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data from r/changemyview using Reddit's API through the PRAW library\n",
    "# Randomly select pairs of sentences from neighbouring \n",
    "# Saves the data to a file for labelling\n",
    "\n",
    "import json\n",
    "import re\n",
    "import praw\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "# download dataset for sentence tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create new praw instance with credentials from praw.ini\n",
    "reddit = praw.Reddit('arg-mining')\n",
    "\n",
    "pairs = []\n",
    "\n",
    "ap = ArgumentPredictor()\n",
    "\n",
    "# Add sentences from submission title, body and comments to arg_threads array\n",
    "for submission in reddit.subreddit('changemyview').hot(limit=200):\n",
    "       \n",
    "    # Remove \"replace more\" from comments results (expand full comment tree)\n",
    "    submission.comments.replace_more(limit=50)\n",
    "    \n",
    "    # Get full comment tree under top level comments and add to arg_threads\n",
    "    for comment in submission.comments.list():\n",
    "       \n",
    "        # All arg sentences in the current comment\n",
    "        comment_sentences = [sentence for sentence in nltk.sent_tokenize(comment.body)]\n",
    "        comment_sentences = clean_sentences(comment_sentences)\n",
    "        comment_sentences = [sentence for sentence in comment_sentences if ap.is_arg(sentence)]\n",
    "        \n",
    "        # All arg sentences in all replies to the current comment\n",
    "        reply_sentences = [sentence for reply in comment.replies.list() for sentence in nltk.sent_tokenize(reply.body) ]\n",
    "        reply_sentences = clean_sentences(reply_sentences)\n",
    "        reply_sentences = [sentence for sentence in reply_sentences if ap.is_arg(sentence)]\n",
    "        \n",
    "        if reply_sentences:\n",
    "            for comment_sentence in comment_sentences:\n",
    "                reply_sentence = random.choice(reply_sentences)\n",
    "                pairs.append('£££££££'.join([comment_sentence, reply_sentence]))\n",
    "    \n",
    "pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the pairs by taking a sample of the entire list\n",
    "pairs_shuffled = random.sample(pairs, len(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./unlabelled_data/unlabelled_relations_sentences.txt', 'w') as write_file:\n",
    "    for pair in pairs_shuffled:\n",
    "        write_file.write(pair + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
