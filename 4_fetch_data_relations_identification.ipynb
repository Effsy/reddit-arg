{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data\n",
    "\n",
    "# \n",
    "# All arguments in a thread\n",
    "\n",
    "# get all sentences\n",
    "\n",
    "# Either (Can also prune the comment trees before any of these):\n",
    "# Pair all sentences n^2\n",
    "# Pair some sentences (randomly)\n",
    "# Pair sentences only between comment replies\n",
    "\n",
    "# Predict the relations between all.\n",
    "\n",
    "# Connect into graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/effsy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/effsy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Retrieve data from r/changemyview using Reddit's API through the PRAW library\n",
    "# Randomly select pairs of sentences from neighbouring \n",
    "# Saves the data to a file for labelling\n",
    "\n",
    "import json\n",
    "import re\n",
    "import praw\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# Create new praw instance with credentials from praw.ini\n",
    "reddit = praw.Reddit('arg-mining')\n",
    "\n",
    "pairs = []\n",
    "\n",
    "ap = ArgumentPredictor()\n",
    "\n",
    "# Add sentences from submission title, body and comments to arg_threads array\n",
    "for submission in reddit.subreddit('changemyview').hot(limit=200):\n",
    "       \n",
    "    # Remove \"replace more\" from comments results (expand full comment tree)\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    \n",
    "    # Get full comment tree under top level comments and add to arg_threads\n",
    "    for comment in submission.comments.list():\n",
    "       \n",
    "        # All arg sentences in the current comment\n",
    "        comment_sentences = [sentence for sentence in nltk.sent_tokenize(comment.body) if ap.is_arg(sentence)]\n",
    "        \n",
    "        # All arg sentences in all replies to the current comment\n",
    "        reply_sentences = [sentence for reply in comment.replies.list() for sentence in nltk.sent_tokenize(reply.body) if ap.is_arg(sentence)]\n",
    "        \n",
    "        if reply_sentences:\n",
    "            for comment_sentence in comment_sentences:\n",
    "                reply_sentence = random.choice(reply_sentences)\n",
    "                pairs.append('£££££££'.join([comment_sentence, reply_sentence]))\n",
    "            \n",
    "pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shuffle the pairs by taking a sample of the entire list\n",
    "pairs_shuffled = random.sample(pairs, len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset for sentence tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/effsy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from importlib import reload\n",
    "\n",
    "from argument_detection import ArgumentPredictor\n",
    "\n",
    "reload(argument_detection)\n",
    "\n",
    "argument_predictor = ArgumentPredictor()\n",
    "argument_predictor.predict_argument(\"I would tell you that it is more of a follow on to the actions of Obama.\")\n",
    "argument_predictor.is_arg(\"I would tell you that it is more of a follow on to the actions of Obama.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize all words in a sentence. Uses PoS to identify lemma\n",
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        else:\n",
    "            yield word\n",
    "\n",
    "# Extract features from a dataset\n",
    "def extract_features(data_x):\n",
    "    \n",
    "    # Predictions based on ngrams from classifier trained on all data\n",
    "    ngram_predictions = [1 if prediction == \"arg\" else 0 for prediction in clf_svm.predict(data_x)]\n",
    "    \n",
    "    # Sentiment of sentence\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment = [sid.polarity_scores(sample)['compound'] for sample in data_x]\n",
    "    \n",
    "    # Sentence Length\n",
    "    sentence_length = [len(word_tokenize(sample)) for sample in data_x]\n",
    "    \n",
    "    # Number of each parts of speech\n",
    "    pos_counts = []\n",
    "    for sample in data_x:\n",
    "        nn = 0\n",
    "        vb = 0\n",
    "        jj = 0\n",
    "        for word, pos in pos_tag(word_tokenize(sample)):\n",
    "            if pos.startswith('NN'):\n",
    "                nn += 1\n",
    "            elif pos.startswith('VB'):\n",
    "                vb += 1\n",
    "            elif pos.startswith('JJ'):\n",
    "                jj += 1\n",
    "        pos_counts.append([nn, vb, jj])\n",
    "    \n",
    "    # Combine features into numpy matrix\n",
    "    return np.column_stack([ngram_predictions, sentiment, sentence_length, pos_counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_args(text):\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import string\n",
    "sentence = \"sadsad&**\"\n",
    "\n",
    "sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "string.punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/effsy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['not_arg'], dtype='<U7')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not_arg'], dtype='<U7')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "dataset = []\n",
    "\n",
    "with open('./labelled_data/1000_labelled_argument_sentences_3.json') as f:\n",
    "    for line in f:\n",
    "        json_line = json.loads(line)\n",
    "        arg = {\"text\": json_line[\"content\"], \"label\": json_line[\"annotation\"][\"labels\"][0]}\n",
    "\n",
    "        dataset.append(arg)\n",
    "\n",
    "dataset\n",
    "\n",
    "# Remove punctuation\n",
    "\n",
    "import string\n",
    "\n",
    "dataset = [{\"text\": sample[\"text\"].translate(str.maketrans('', '', string.punctuation)), \"label\":sample[\"label\"]} for sample in dataset]\n",
    "dataset\n",
    "\n",
    "\n",
    "data = [sample[\"text\"] for sample in dataset]\n",
    "data\n",
    "argument_predictor.predict_argument('The motivation for the age restriction')\n",
    "# [argument_predictor.predict_argument(text) for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lemma = \"People having good careers, means more revenue for the government and happier citizens.\"\n",
    "\n",
    "1 if ngram_model.predict([sentence_lemma])[0] == \"arg\" else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object lemmatize_sentence at 0x7f7eb60e1678>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        else:\n",
    "            yield word\n",
    "\n",
    "lemmatize_sentence(\"People having good careers, means more revenue for the government and happier citizens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not_arg'], dtype='<U7')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argument_predictor.predict_argument(\"People having good careers, means more revenue for the government and happier citizens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-2584252fd132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeta_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "meta_model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "def lemmatize_sentence(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemma_array = []\n",
    "\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            lemma_array.append(wnl.lemmatize(word, pos='n'))\n",
    "        elif tag.startswith('VB'):\n",
    "            lemma_array.append(wnl.lemmatize(word, pos='v'))\n",
    "        elif tag.startswith('JJ'):\n",
    "            lemma_array.append(wnl.lemmatize(word, pos='a'))\n",
    "        else:\n",
    "            lemma_array.append(word)\n",
    "        \n",
    "    return ' '.join(lemma_array)\n",
    "\n",
    "def extract_features(sentence):\n",
    "    # Predictions based on ngrams from classifier trained on all data       \n",
    "\n",
    "    # Lemmatize\n",
    "    sentence_lemma = [lemmatize_sentence(sentence)]\n",
    "\n",
    "    ngram_prediction = 1 if ngram_model.predict(sentence_lemma)[0] == \"arg\" else 0\n",
    "\n",
    "    # Sentiment of sentence\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment = sid.polarity_scores(sentence)['compound']\n",
    "\n",
    "    # Sentence Length\n",
    "    sentence_length = len(word_tokenize(sentence))\n",
    "\n",
    "    # Number of each parts of speech\n",
    "    pos_counts = []\n",
    "    nn = 0\n",
    "    vb = 0\n",
    "    jj = 0\n",
    "    for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "        if pos.startswith('NN'):\n",
    "            nn += 1\n",
    "        elif pos.startswith('VB'):\n",
    "            vb += 1\n",
    "        elif pos.startswith('JJ'):\n",
    "            jj += 1\n",
    "    pos_counts.append([nn, vb, jj])\n",
    "\n",
    "    # Combine features into numpy matrix\n",
    "    return np.column_stack([ngram_prediction, sentiment, sentence_length, pos_counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "extract_features(\"People having good careers, means more revenue for the government and happier citizens.\")\n",
    "data[:10]\n",
    "\n",
    "# [extract_features(text) for text in data[:209]]\n",
    "\n",
    "data[208]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_sentence(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
