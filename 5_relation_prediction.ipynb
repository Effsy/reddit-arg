{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8s_ll-EdB7DD"
   },
   "source": [
    "# Relation Prediction\n",
    "\n",
    "In this notebook, the relation prediction model is trained. \n",
    "\n",
    "This notebook should be run in Google Colab\n",
    "\n",
    "Data preprocessing and model implementation loosely inspired from: https://keras.io/examples/pretrained_word_embeddings/\n",
    "\n",
    "Note: The entire notebook should not be run sequentially. Each section should be run against the data separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 151473,
     "status": "ok",
     "timestamp": 1587265005262,
     "user": {
      "displayName": "Jardin Omidvaran",
      "photoUrl": "",
      "userId": "10348469921877125146"
     },
     "user_tz": -60
    },
    "id": "69x_yLAXKJGZ",
    "outputId": "e4f7fc1e-f5ca-4243-dbc8-f8b71ff3db84"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61407,
     "status": "ok",
     "timestamp": 1587265073178,
     "user": {
      "displayName": "Jardin Omidvaran",
      "photoUrl": "",
      "userId": "10348469921877125146"
     },
     "user_tz": -60
    },
    "id": "gXW7yT0weiTx",
    "outputId": "09ab7d59-fb7e-4c9e-df57-41cd46cbc29e"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.0.0-alpha0\n",
    "!pip install keras-tuner\n",
    "!pip install scikit-learn\n",
    "\n",
    "# Model building/processing\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Merge layers\n",
    "from tensorflow.keras.layers import Add, Concatenate\n",
    "\n",
    "# split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# hyperparameter tuning\n",
    "from kerastuner import RandomSearch, Objective\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Baseline - Dummy classifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# saving/loading\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "import os, datetime\n",
    "import sys\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57_rCCe0HRvE"
   },
   "outputs": [],
   "source": [
    "# Embedding parameters\n",
    "GLOVE_DIR = '/content/drive/My Drive/Colab Notebooks/glove'\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Data parameters\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NUM_WORDS = 50000\n",
    "\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3308,
     "status": "ok",
     "timestamp": 1587265098611,
     "user": {
      "displayName": "Jardin Omidvaran",
      "photoUrl": "",
      "userId": "10348469921877125146"
     },
     "user_tz": -60
    },
    "id": "GXQzNAZFJhOf",
    "outputId": "3471f5c5-e6d3-43c4-f75f-e1e381943b25"
   },
   "outputs": [],
   "source": [
    "# embeddings for entire dataset\n",
    "\n",
    "#load glove embedding into a dict\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        if len(embeddings_index) >= MAX_NUM_WORDS:\n",
    "            break\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        value = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = value\n",
    "\n",
    "word_index = {w: i for i, w in enumerate(embeddings_index.keys(), 1)}\n",
    "\n",
    "#create embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector[:EMBEDDING_DIM]\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j5-VvTnImWHF"
   },
   "outputs": [],
   "source": [
    "# sentence tokenizer\n",
    "\n",
    "def texts_to_sequences(texts, word_index):\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        tokens = text_to_word_sequence(text)\n",
    "        sequences.append([word_index.get(w) for w in tokens if w in word_index])\n",
    "    return sequences\n",
    "\n",
    "def text_to_sequence(text, word_index):\n",
    "    tokens = text_to_word_sequence(text)\n",
    "    return [word_index.get(w) for w in tokens if w in word_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12913,
     "status": "ok",
     "timestamp": 1587265113034,
     "user": {
      "displayName": "Jardin Omidvaran",
      "photoUrl": "",
      "userId": "10348469921877125146"
     },
     "user_tz": -60
    },
    "id": "M6lVBw2XzBvD",
    "outputId": "e5ece42d-0750-4684-d142-4c3516606410"
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "\n",
    "dataset = []\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/labelled_data/relations_dataset.json') as f:\n",
    "    for line in f:\n",
    "        json_line = json.loads(line)\n",
    "        print(line)\n",
    "        arg = {\"text\": json_line[\"content\"], \"label\": json_line[\"annotation\"][\"labels\"][0]}\n",
    "\n",
    "        dataset.append(arg)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Hpm3yaCAaka"
   },
   "outputs": [],
   "source": [
    "# Prepare samples and labels\n",
    "\n",
    "# Two sample text sets are required for originator and responder\n",
    "texts_originator = []\n",
    "texts_responder = []\n",
    "\n",
    "labels_index = {'attack':0, 'support':1, 'neither':1}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for sample in dataset:\n",
    "    texts = sample['text'].split('£££££££')\n",
    "    texts_originator.append(texts[0])\n",
    "    texts_responder.append(texts[1])\n",
    "\n",
    "labels = [labels_index[sample[\"label\"]] for sample in dataset]\n",
    "\n",
    "# Combined text sets\n",
    "texts = texts_responder + texts_originator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 667,
     "status": "ok",
     "timestamp": 1587265114296,
     "user": {
      "displayName": "Jardin Omidvaran",
      "photoUrl": "",
      "userId": "10348469921877125146"
     },
     "user_tz": -60
    },
    "id": "D3dsaIg5MTCQ",
    "outputId": "03873243-8580-4a7b-a38f-8939c6bf3a5a"
   },
   "outputs": [],
   "source": [
    "originator_sequences = texts_to_sequences(texts_originator, word_index)\n",
    "responder_sequences = texts_to_sequences(texts_responder, word_index)\n",
    "\n",
    "originator_data = pad_sequences(originator_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "responder_data = pad_sequences(responder_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Convert labels to one-hot encoded matrix\n",
    "labels_data = np.array(labels)\n",
    "print('Shape of originator data tensor:', originator_data.shape)\n",
    "print('Shape of responder_data tensor:', responder_data.shape)\n",
    "print('Shape of label tensor:', labels_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CsbThHJ35s3k"
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "\n",
    "x_originator_data, x_originator_test, x_responder_data, x_responder_test, y_data, y_test = train_test_split(originator_data, responder_data, labels_data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1587265118640,
     "user": {
      "displayName": "Jardin Omidvaran",
      "photoUrl": "",
      "userId": "10348469921877125146"
     },
     "user_tz": -60
    },
    "id": "rxFT3RGTb1hW",
    "outputId": "4da9c727-14d8-4d70-f776-7895e0688a1c"
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "\n",
    "num_validation_samples = int(VALIDATION_SPLIT * len(x_originator_data))\n",
    "\n",
    "x_originator_train = x_originator_data[:-num_validation_samples]\n",
    "x_responder_train = x_responder_data[:-num_validation_samples]\n",
    "y_train = y_data[:-num_validation_samples]\n",
    "\n",
    "x_originator_val = x_originator_data[-num_validation_samples:]\n",
    "x_responder_val = x_responder_data[-num_validation_samples:]\n",
    "y_val = y_data[-num_validation_samples:]\n",
    "\n",
    "print(x_originator_train[:2])\n",
    "print(x_responder_train[:2])\n",
    "print(y_train[:2])\n",
    "print(x_originator_val[:2])\n",
    "print(x_responder_val[:2])\n",
    "print(y_val[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CyeHqCjVni77"
   },
   "source": [
    "## Training (Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkDR_DY6iLZl"
   },
   "outputs": [],
   "source": [
    "hyperparamter_search_results = []\n",
    "\n",
    "# get best model\n",
    "best_model_f1_val = 0\n",
    "best_model = None\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "    def __init__(self, validation_data, test_data):\n",
    "        self.validation_data = validation_data\n",
    "        self.test_data = test_data\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Calculate f1 score on test set\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        \n",
    "        val_f1 = f1_score(val_targ, val_predict, average='macro')\n",
    "\n",
    "        # save best model only\n",
    "        global best_model_f1_val\n",
    "        global best_model\n",
    "        if val_f1 > best_model_f1_val:\n",
    "            best_model_f1_val = val_f1\n",
    "            best_model = self.model\n",
    "\n",
    "            test_predict = (np.asarray(best_model.predict(self.test_data[0]))).round()\n",
    "            test_targ = self.test_data[1]\n",
    "\n",
    "            test_f1 = f1_score(test_targ, test_predict, average='macro')\n",
    "\n",
    "            best_model_info = {\"val_f1\": best_model_f1_val,\n",
    "                            \"test_f1\": test_f1,\n",
    "                            \"model_config\": str(best_model.get_config())\n",
    "                            }\n",
    "            # Save the best model results and model h5 file\n",
    "            with open('/content/drive/My Drive/Colab Notebooks/best_model_results_95.json', 'w') as f:\n",
    "                json.dump(best_model_info, f)\n",
    "\n",
    "            best_model.save('/content/drive/My Drive/Colab Notebooks/best_model_95.h5')\n",
    "\n",
    "        return\n",
    " \n",
    "metrics = Metrics(validation_data=[[x_originator_val, x_responder_val], y_val], test_data=[[x_originator_test, x_responder_test], y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gjZNBoHAZogB",
    "outputId": "8b9d5226-9bad-4728-bbce-dbf10a08487b"
   },
   "outputs": [],
   "source": [
    "best_model_f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "X4W_DREjH2Xl",
    "outputId": "d25da42b-fdb2-46b7-82e9-4e80038c23d8"
   },
   "outputs": [],
   "source": [
    "# Siamese network loosely inspired by: https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d\n",
    "\n",
    "def build_model(hp):\n",
    "    lstm_units = hp.Int(\"lstm\", min_value=8, max_value=64, step=8)\n",
    "    \n",
    "    # Additional hyperparameters\n",
    "    # lstm_recurrent_dropout = hp.Float(\"lstm_recurrent_dropout\", min_value=0.0, max_value=0.9, step=0.1)\n",
    "    # lstm_dropout = hp.Float(\"lstm_dropout\", min_value=0.0, max_value=0.9, step=0.1)\n",
    "\n",
    "    # Branch 1\n",
    "    model_1 = Sequential()\n",
    "    model_1.add(Embedding(embedding_matrix.shape[0],\n",
    "                        embedding_matrix.shape[1],\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        input_length=MAX_SEQUENCE_LENGTH,\n",
    "                        trainable=False))\n",
    "    \n",
    "    model_1.add(LSTM(lstm_units))\n",
    "    \n",
    "    # Branch 2\n",
    "    model_2 = Sequential()\n",
    "    model_2.add(Embedding(embedding_matrix.shape[0],\n",
    "                        embedding_matrix.shape[1],\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        input_length=MAX_SEQUENCE_LENGTH,\n",
    "                        trainable=False))\n",
    "    model_2.add(LSTM(lstm_units))\n",
    "    if hp.Choice('merge', ['cat', 'add']) == 'cat':\n",
    "        conc = Concatenate()([model_1.output, model_2.output])\n",
    "    else:\n",
    "        conc = Add()([model_1.output, model_2.output])\n",
    "\n",
    "    out = Dense(hp.Int(\"dense\", min_value=8, max_value=64, step=8), activation='relu')(conc)\n",
    "    out = Dropout(hp.Float(\"dense_dropout\", min_value=0.0, max_value=0.9, step=0.1))(out)\n",
    "    out = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "    # Connect the inputs with the outputs\n",
    "    model = Model([model_1.input, model_2.input], out)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=Adam(\n",
    "                    hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "                metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(build_model,\n",
    "                     objective=Objective('val_acc', direction=\"max\"),\n",
    "                     max_trials=50,\n",
    "                     executions_per_trial=1,\n",
    "                     directory=\"log\",\n",
    "                     project_name=datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "tuner.search(x=[x_originator_train, x_responder_train],\n",
    "             y=y_train,\n",
    "             epochs=5,\n",
    "             batch_size=64,\n",
    "             validation_data=([x_originator_val, x_responder_val], y_val),\n",
    "             callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZEt-sxA4CIaD"
   },
   "source": [
    "## Training the best model\n",
    "\n",
    "This uses the best hyperparameters and aims to find the best number of epochs to train for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ymIMUs6YcV4"
   },
   "outputs": [],
   "source": [
    "# Save val f1, test f1, loss and val loss for analysis\n",
    "f1_score_epochs = []\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "    def __init__(self, validation_data, test_data):\n",
    "        self.validation_data = validation_data\n",
    "        self.test_data = test_data\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        # Calculate f1 score on test set\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        \n",
    "        val_f1 = f1_score(val_targ, val_predict, average='macro')\n",
    "\n",
    "        test_predict = (np.asarray(self.model.predict(self.test_data[0]))).round()\n",
    "        test_targ = self.test_data[1]\n",
    "\n",
    "        test_f1 = f1_score(test_targ, test_predict, average='macro')\n",
    "        \n",
    "        f1_score_epochs.append([val_f1, test_f1, logs.get('loss'), logs.get('val_loss')])\n",
    "\n",
    "        return\n",
    " \n",
    "metrics = Metrics(validation_data=[[x_originator_val, x_responder_val], y_val], test_data=[[x_originator_test, x_responder_test], y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50896,
     "status": "ok",
     "timestamp": 1587269447307,
     "user": {
      "displayName": "Jardin Omidvaran",
      "photoUrl": "",
      "userId": "10348469921877125146"
     },
     "user_tz": -60
    },
    "id": "rb59HjMDCRc6",
    "outputId": "43b08d4f-d106-4046-b4e1-d8aae4f56790"
   },
   "outputs": [],
   "source": [
    "# Siamese network loosely inspired by: https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d\n",
    "\n",
    "# Branch 1\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(embedding_matrix.shape[0],\n",
    "                    embedding_matrix.shape[1],\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "\n",
    "model_1.add(LSTM(units=8))\n",
    "\n",
    "# Branch 2\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(embedding_matrix.shape[0],\n",
    "                    embedding_matrix.shape[1],\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "model_2.add(LSTM(units=8))\n",
    "conc = Concatenate()([model_1.output, model_2.output])\n",
    "out = Dense(32)(conc)\n",
    "out = Dropout(0.1)(out)\n",
    "out = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "# Connect the inputs with the outputs\n",
    "model = Model([model_1.input, model_2.input], out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer=Adam(),\n",
    "            metrics=['acc'])\n",
    "\n",
    "history_callback = model.fit([x_originator_train, x_responder_train], y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          validation_data=([x_originator_val, x_responder_val], y_val),\n",
    "          callbacks=[metrics])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U11QdoNHCbof"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/My Drive/Colab Notebooks/best_model_all_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1587269488367,
     "user": {
      "displayName": "Jardin Omidvaran",
      "photoUrl": "",
      "userId": "10348469921877125146"
     },
     "user_tz": -60
    },
    "id": "1FeihMkCa9T5",
    "outputId": "06fe8902-25d5-4919-f0ae-83f5d18b02b8"
   },
   "outputs": [],
   "source": [
    "f1_score_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUsmw6elIqhg"
   },
   "source": [
    "## Baseline\n",
    "\n",
    "The dummy classifiers ignore the data and predict using just the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "3JQR6EDaJm8G",
    "outputId": "2f2633f4-7a82-4e0b-9628-b0d670068409"
   },
   "outputs": [],
   "source": [
    "# Dummy classifier - 2 label\n",
    "\n",
    "parameters_dum = {'strategy': ['stratified', 'most_frequent', 'prior', 'uniform']}\n",
    "\n",
    "clf_dum = GridSearchCV(DummyClassifier(), parameters_dum, cv=StratifiedKFold(n_splits=3, random_state=999), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "clf_dum.fit(x_originator_data, y_data)\n",
    "\n",
    "x_originator_test\n",
    "\n",
    "clf_dum.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "QGDRyxqmIkrg",
    "outputId": "c996d2ea-bb87-43df-f79a-f1fafbe82bd7"
   },
   "outputs": [],
   "source": [
    "# Dummy classifier score\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Dummy\")\n",
    "print(\"Test Score: \" + str(metrics.f1_score(y_test, clf_dum.predict(x_originator_test), average='macro')))\n",
    "print(\"Best Score: \" + str(clf_dum.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "futMp7woVmqL",
    "outputId": "fcde4d2c-e62c-45d9-c686-260debf02927"
   },
   "outputs": [],
   "source": [
    "# Dummy classifier - 3 label\n",
    "# NOTE: Make sure the labels_index is changed to contain 3 labels\n",
    "\n",
    "parameters_dum = {'strategy': ['stratified', 'most_frequent', 'prior', 'uniform']}\n",
    "\n",
    "clf_dum = GridSearchCV(DummyClassifier(), parameters_dum, cv=StratifiedKFold(n_splits=3, random_state=999), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "clf_dum.fit(x_originator_data, y_data)\n",
    "\n",
    "x_originator_test\n",
    "\n",
    "clf_dum.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "EvrA2JaTVnlQ",
    "outputId": "53579daa-78cc-48fd-b54a-f79c7305870e"
   },
   "outputs": [],
   "source": [
    "# Dummy classifier score\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Dummy\")\n",
    "print(\"Test Score: \" + str(metrics.f1_score(y_test, clf_dum.predict(x_originator_test), average='macro')))\n",
    "print(\"Best Score: \" + str(clf_dum.best_score_))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "5_relation_prediction.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
